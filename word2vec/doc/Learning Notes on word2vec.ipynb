{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [word2vec词向量训练及中文文本相似度计算](http://blog.csdn.net/eastmount/article/details/50637476)\n",
    "1.词向量是多维实数向量，**向量中包含了自然语言中的语义和语法关系，词向量之间余弦距离的大小代表了词语之间关系的远近，词向量的加减运算则是计算机在\"遣词造句\"**。  \n",
    "2.词向量具有良好的语义特性，是表示词语特征的常用方式。**词向量的每一维的值代表一个具有一定的语义和语法上解释的特征**。故可以将词向量的每一维称为一个词语特征。词向量用Distributed Representation表示，一种低维实数向量。  \n",
    "**Distributed Representation低维实数向量，如：[0.792, −0.177, −0.107, 0.109, −0.542, …]。它让相似或相关的词在距离上更加接近。**  \n",
    "3.Word2vec是Google公司在2013年开放的一款用于训练词向量的软件工具。它根据给定的语料库，通过优化后的训练模型快速有效的将一个词语表达成向量形式，其核心架构包括**CBOW和Skip-gram**。  \n",
    "4.NNLM模型是神经网络概率语言模型的基础模型。在NNLM模型中，**从隐含层到输出层的计算是主要影响训练效率的地方，CBOW和Skip-gram模型考虑去掉隐含层。实践证明新训练的词向量的精确度可能不如NNLM模型（具有隐含层），但可以通过增加训练语料的方法来完善**。  \n",
    "5.Word2vec包含两种训练模型，分别是CBOW和Skip_gram(输入层、发射层、输出层)，如下图所示：  \n",
    "![CBOW_vs_Skip-gram](./CBOW_vs_Skip-gram.jpg)  \n",
    "6.**CBOW模型**理解为上下文决定当前词出现的概率。在CBOW模型中，上下文所有的词对当前词出现概率的影响的权重是一样的，因此叫**CBOW(continuous bag-of-words model)模型**。如在袋子中取词，取出数量足够的词就可以了，至于取出的先后顺序是无关紧要的。  \n",
    "7.**Skip-gram模型**是一个简单实用的模型。为什么会提出该问题呢？\n",
    "在NLP中，语料的选取是一个相当重要的问题。  \n",
    "首先，语料必须充分。一方面词典的词量要足够大，另一方面尽可能地包含反映词语之间关系的句子，如“鱼在水中游”这种句式在语料中尽可能地多，模型才能学习到该句中的语义和语法关系，这和人类学习自然语言是一个道理，重复次数多了，也就会模型了。  \n",
    "其次，语料必须准确。所选取的语料能够正确反映该语言的语义和语法关系。如中文的《人民日报》比较准确。但更多时候不是语料选取引发准确性问题，而是处理的方法。  \n",
    "由于窗口大小的限制，这会导致**超出窗口的词语与当前词之间的关系不能正确地反映到模型中，如果单纯扩大窗口大小会增加训练的复杂度。Skip-gram模型的提出很好解决了这些问题。**  \n",
    "8.Skip-gram表示“跳过某些符号”。例如句子“中国/足球/踢得/真是/太烂/了”有4个3元词组，分别是“中国足球踢得”、“足球踢得真是”、“踢得真是太烂”、“真是太烂了”，句子的本意都是“中国足球太烂”，可是上面4个3元组并不能反映出这个信息。  \n",
    "此时，使用Skip-gram模型允许某些词被跳过，因此可组成“中国足球太烂”这个3元词组。如果允许跳过2个词，即2-Skip-gram，那么上句话组成的3元词组如下表所示:  \n",
    "![Skip-gram_demo](./Skip-gram_demo.jpg)  \n",
    "由上表可知:**一方面Skip-gram反映了句子的真实意思，在新组成的这18个3元词组中，有8个词组能够正确反映例句中的真实意思**；另一方面，**扩大了语料**，3元词组由原来的4个扩展到了18个。语料的扩展能够提高训练的准确度，获得的词向量更能反映真实的文本含义。  \n",
    "9.word2vec参数说明如下图所示:\n",
    "![word2vec参数说明](word2vec参数说明.png)\n",
    "\n",
    "## [word2vec: https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/)\n",
    "\n",
    "1.The word2vec tool takes a text corpus as input and produces the word vectors as output. It first constructs a vocabulary from the training text data and then learns vector representation of words.  \n",
    "2.There are two main learning algorithms in word2vec : **continuous bag-of-words** and **continuous skip-gram**.  \n",
    "3.**It was recently shown that the word vectors capture many _linguistic regularities_, for example vector operations vector('Paris') - vector('France') + vector('Italy') results in a vector that is very close to vector('Rome'), and vector('king') - vector('man') + vector('woman') is close to vector('queen')**.  \n",
    "To observe strong regularities in the word vector space, **it is needed to train the models on large data set, with sufficient vector dimensionality as shown in [1].**  \n",
    "4.**The linearity of the vector operations seems to weakly hold also for the addition of several vectors, so it is possible to add several word or phrase vectors to form representation of short sentences.**[2]  \n",
    "5.**How to measure quality of the word vectors**  \n",
    "Several factors influence the quality of the word vectors:\n",
    "* amount and quality of the training data\n",
    "* size of the vectors\n",
    "* training algorithm\n",
    "\n",
    "The quality of the vectors is crucial for any application. However, exploration of different hyper-parameter settings for complex tasks might be **too time demanding**. Thus, **we designed simple test sets that can be used to quickly evaluate the word vector quality**.  \n",
    "For the word relation test set described in [1], see `./demo-word-accuracy.sh`, for the phrase relation test set described in [2], see `./demo-phrase-accuracy.sh`. Note that the accuracy depends heavily on the amount of the training data; our best results for both test sets are above 70% accuracy with coverage close to 100%.  \n",
    "6.**Performance**  \n",
    "The training speed can be significantly improved by using parallel training on multiple-CPU machine (use the switch `-threads N`). The hyper-parameter choice is crucial for performance (both speed and accuracy), however varies for different applications. The main choices to make are:  \n",
    "* **architecture**: skip-gram (slower, better for infrequent words) vs CBOW (fast)\n",
    "* **the training algorithm**: hierarchical softmax (better for infrequent words) vs negative sampling (better for frequent words, better with low dimensional vectors)\n",
    "* **sub-sampling of frequent words**: can improve both accuracy and speed for large data sets (useful values are in range 1e-3 to 1e-5)\n",
    "* **dimensionality of the word vectors**: usually more is better, but not always\n",
    "* **context (window) size**: for skip-gram usually around 10, for CBOW around 5\n",
    "\n",
    "7.**duplicate sentences should be removed before training the models**\n",
    "\n",
    "####  References\n",
    "[1] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf). In Proceedings of Workshop at ICLR, 2013.\n",
    "[2] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf). In Proceedings of NIPS, 2013."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
